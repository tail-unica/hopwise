{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook creates the embeddings using Sentence Transformers for Semantic IDs.\n",
    "\n",
    "*In particular, it loads the .item file, creates a template to be fed to a sentence transformer and then save the embedding for each item in a .itememb file.*\n",
    "\n",
    "This notebook currently support:\n",
    "- RPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set the configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/recsysdatasets\"\n",
    "dataset = \"ml-1m_small\"\n",
    "sentence_trf_bs = 2048 # set the highest bs that fits in your GPU memory\n",
    "sentence_trf_model = \"sentence-transformers/sentence-t5-base\" # size 768\n",
    "apply_pca = True\n",
    "sentence_trf_pca_components = 128\n",
    "openai_api_key = None\n",
    "separator = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**specify which columns to use as metadata**\n",
    "\n",
    "\n",
    "*they depend on the dataset. In this case, it's for ml-100k.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la colonna feature non l'ho trovata nei metadata amazon.\n",
    "columns_to_concatenate = ['movie_title', 'release_year', 'class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set the device used by the sentence transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load .item file used to retrieve item's metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat_file = os.path.join(data_path, dataset, f'{dataset}.item')\n",
    "item_feat_df = pd.read_csv(item_feat_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rename the columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns by removing everything after ':'\n",
    "new_columns = {}\n",
    "for col in item_feat_df.columns:\n",
    "    new_columns[col] = col.split(':')[0]\n",
    "item_feat_df.rename(columns=new_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**concatenate values and create the final dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities functions used to preprocess the feature data of an item.\n",
    "def list_to_str(l: Union[list, str], remove_blank=False) -> str:\n",
    "    \"\"\"\n",
    "    Converts a list or a string to a string representation.\n",
    "\n",
    "    Args:\n",
    "        l (Union[list, str]): The input list or string.\n",
    "\n",
    "    Returns:\n",
    "        str: The string representation of the input.\n",
    "    \"\"\"\n",
    "    ret = ''\n",
    "    if isinstance(l, list):\n",
    "        ret = ', '.join(map(str, l))\n",
    "    else:\n",
    "        ret = l\n",
    "    if remove_blank:\n",
    "        ret = ret.replace(' ', '')\n",
    "    return ret\n",
    "\n",
    "def clean_text(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the raw text by removing HTML tags, special characters, and extra spaces.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): The raw text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    text = list_to_str(raw_text)\n",
    "    text = html.unescape(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'[\\n\\t]', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "    return text\n",
    "\n",
    "def _sent_process(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Process the raw input according to the raw data type and return a processed sentence.\n",
    "\n",
    "    Args:\n",
    "        raw (str): The raw input to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed sentence.\n",
    "    \"\"\"\n",
    "    sentence = \"\"\n",
    "    if isinstance(raw, float):\n",
    "        sentence += str(raw)\n",
    "        sentence += '.'\n",
    "    elif len(raw) > 0 and isinstance(raw[0], list):\n",
    "        for v1 in raw:\n",
    "            for v in v1:\n",
    "                sentence += clean_text(v)[:-1]\n",
    "                sentence += ', '\n",
    "        sentence = sentence[:-2]\n",
    "        sentence += '.'\n",
    "    elif isinstance(raw, list):\n",
    "        for v1 in raw:\n",
    "            sentence += clean_text(v1)\n",
    "    else:\n",
    "        sentence = clean_text(raw)\n",
    "    return sentence + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'amazon' in dataset.lower():\n",
    "    item_dict = dict()\n",
    "    for index, row in item_feat_df.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        meta_sentence = ''\n",
    "        # Iterate through each column identified for concatenation\n",
    "        for col in columns_to_concatenate:\n",
    "            # Convert each value to string and handle potential NaN values by replacing them with an empty string\n",
    "            meta_sentence += _sent_process(row[col])\n",
    "        \n",
    "        # Join the collected parts with a space and remove any leading/trailing whitespace\n",
    "        item_dict[item_id] = meta_sentence\n",
    "else:\n",
    "    item_dict = dict()\n",
    "    for index, row in item_feat_df.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        concatenated_parts = []\n",
    "        # Iterate through each column identified for concatenation\n",
    "        for col in columns_to_concatenate:\n",
    "            # Convert each value to string and handle potential NaN values by replacing them with an empty string\n",
    "            value = row[col] if pd.notna(row[col]) else ''\n",
    "            concatenated_parts.append(value)\n",
    "\n",
    "        # convert everything to string\n",
    "        concatenated_parts = [str(part) for part in concatenated_parts]\n",
    "        # Join the collected parts with a space and remove any leading/trailing whitespace  \n",
    "        item_dict[item_id] = separator.join(concatenated_parts).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**item dict is a dictionary where the key is the item and the value is the description that will be used to obtain from the sentence trasformer the corresponding representation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Sentence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sentence-transformer' in sentence_trf_model:\n",
    "    print(f\"Using {sentence_trf_model} for sentence embeddings\")\n",
    "    sentence_trf = SentenceTransformer(sentence_trf_model).to(device)\n",
    "\n",
    "    sentence_embeddings = sentence_trf.encode(\n",
    "        list(item_dict.values()),\n",
    "        convert_to_numpy=True,\n",
    "        batch_size=sentence_trf_bs,\n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "else:\n",
    "    print(f\"Using OpenAI {sentence_trf_model} for sentence embeddings\")\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    sentence_embeddings = []\n",
    "    for i in tqdm(range(0, len(item_dict.values()), sentence_trf_bs), desc='Encoding: '):\n",
    "        try:\n",
    "            responses = client.embeddings.create(\n",
    "                input=list(item_dict.values())[i: i + sentence_trf_bs],\n",
    "                model=sentence_trf_model\n",
    "            )\n",
    "        except:\n",
    "            print(f'Failed to encode sentence embeddings for {i} - {i + sentence_trf_bs}')\n",
    "            batch = list(item_dict.values())[\n",
    "                i: i + sentence_trf_bs]\n",
    "\n",
    "            new_batch = []\n",
    "            for sent in batch:\n",
    "                \n",
    "                encoding = tiktoken.get_encoding('cl100k_base')\n",
    "                num_tokens = len(encoding.encode(sent))\n",
    "\n",
    "                if num_tokens < 8192:\n",
    "                    new_batch.append(sent)\n",
    "                else:\n",
    "                    n_chars = 8192 / num_tokens * len(sent) - 100\n",
    "                    new_batch.append(sent[:int(n_chars)])\n",
    "\n",
    "            print(f'Retrying with {len(new_batch)} sentences')\n",
    "            responses = client.embeddings.create(\n",
    "                input=new_batch,\n",
    "                model=sentence_trf_model\n",
    "            )\n",
    "\n",
    "        for response in responses.data:\n",
    "            sentence_embeddings.append(response.embedding)\n",
    "    sentence_embeddings = np.array(sentence_embeddings, dtype=np.float32)\n",
    "\n",
    "# for Amazon_Sports_and_Outdoors is about 31minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_pca:\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=sentence_trf_pca_components, whiten=True)\n",
    "    sentence_embeddings = pca.fit_transform(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the representations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'item_embedding_id:token': item_dict.keys(),\n",
    "    'item_embedding:float_seq': sentence_embeddings.tolist(),\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['item_embedding:float_seq'] = df['item_embedding:float_seq'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the final file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(data_path,dataset,f'{dataset}.itememb'),sep=\"\\t\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(data_path,dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
