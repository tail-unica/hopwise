{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Knowledge-Graph Embeddings for Hopwise `dataset.get_preload_weight()` function\n",
    "This notebook shows you how you can format kge methods embeddings to be loaded with `dataset.get_preload_weight`\n",
    "\n",
    "\n",
    "ðŸ“š [Load Pretrained Embedding Documentation](https://recbole.io/docs/user_guide/usage/load_pretrained_embedding.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from hopwise.data import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'saved/TransE-Mar-13-2025_16-18-32.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Embeddings detected are**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you want to exclude some embeddings?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = ['relation_bias_embedding.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Dataset detected is**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = checkpoint['config']['dataset']\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Dataset folder detected is**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = checkpoint['config']['data_path']\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the mappings between embedding and original entity/relation/user**\n",
    "\n",
    "- Users have a mapping 1-1 so we don't need a mapping.\n",
    "\n",
    "- We suppose that indexing starts at 1. (tipically 0 is reserved for [PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(checkpoint['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.field2token_id['tail_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the reverse mapping\n",
    "uid2token = {id: token for token,id in dataset.field2token_id['user_id'].items()}\n",
    "print(uid2token)\n",
    "eid2token = {id: token for token, id in dataset.field2token_id['tail_id'].items()}\n",
    "print(eid2token)\n",
    "rid2token = {id: token for token, id in dataset.field2token_id['relation_id'].items()}\n",
    "print(rid2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add dummy relation, check kge code\n",
    "# rid2token[len(rid2token)] = 'ui_dummy_relation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(eid2token.keys()) == checkpoint['state_dict']['entity_embedding.weight'].shape[0])\n",
    "assert (len(rid2token.keys()) == checkpoint['state_dict']['relation_embedding.weight'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*if the assertion check fails, make sure that you've trained the kge without adding dummy relations/entities explicitly when creating relation/entity embeddings!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_embedding(weight, columns, emb_type):\n",
    "    weight = weight.detach().cpu().numpy()\n",
    "    new_emb_dict = {columns[0]: list(), \n",
    "                    columns[1]: list() }\n",
    "    \n",
    "    if emb_type == 'entity':\n",
    "        mapping = eid2token    \n",
    "    elif emb_type == 'relation':\n",
    "        mapping = rid2token\n",
    "    elif emb_type == 'user':\n",
    "        mapping = uid2token\n",
    "        \n",
    "    # Create index\n",
    "    new_emb_dict[columns[0]] = [mapping[id] if mapping is not None else id for id in range(1,len(weight))]\n",
    "\n",
    "    # Create embedding\n",
    "    new_emb_dict[columns[1]] = [\" \".join(f\"{x}\" for x in row) for row in weight[1:]]\n",
    "    \n",
    "    filename = f'{dataset_name}.{emb_type}emb'\n",
    "    df = pd.DataFrame(new_emb_dict)\n",
    "    print(f\"[+] Saving the new {dataset_name} {columns[0]} embedding in {data_path}/{filename}!\")\n",
    "    df.to_csv(os.path.join(data_path,filename), sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb_name, emb in checkpoint['state_dict'].items():\n",
    "    if emb_name in excluded:\n",
    "        continue\n",
    "    # What is? Entity? User? Relation? Item? \n",
    "    emb_type = emb_name.split(\"_\")[0]\n",
    "    # Create the new embedding file columns\n",
    "    columns = [f'{emb_type}id:token', f'{emb_type}_embedding:float_seq']\n",
    "    print(f\"[+] Formatting {emb_name} with columns {columns}\")\n",
    "    format_embedding(emb, columns, emb_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the dataset folder there are these file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to make sure that the dataset configuration is ok.**\n",
    "\n",
    "Suppose that the output of the format embedding phase is:\n",
    "\n",
    "```text\n",
    "    [+] Formatting user_embedding.weight with columns ['userid:token', 'user_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m userid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.useremb!\n",
    "    [+] Formatting entity_embedding.weight with columns ['entityid:token', 'entity_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m entityid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.entityemb!\n",
    "    [+] Formatting relation_embedding.weight with columns ['relationid:token', 'relation_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m relationid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.relationemb!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you should go to the dataset configuration file (in our case is in `hopwise/properties/dataset/ml-1m.yaml`) and add the new files to be loaded\n",
    "\n",
    "\n",
    "```text\n",
    "    additional_feat_suffix: [useremb, entityemb, relationemb]  \n",
    "    load_col:                                                  \n",
    "        useremb: [userid, user_embedding]\n",
    "        entityemb: [entityid, entity_embedding]\n",
    "        relationemb: [relationid, relation_embedding]\n",
    "    \n",
    "    alias_of_user_id: [userid]\n",
    "    alias_of_entity_id: [entityid]\n",
    "    alias_of_relation_id: [relationid]\n",
    "    \n",
    "    preload_weight:\n",
    "      userid: user_embedding\n",
    "      entityid: entity_embedding\n",
    "      relationid: relation_embedding\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in your code you should be able to access to pretrained embeddings in your model through:\n",
    "\n",
    "*Torch*\n",
    "```python\n",
    "    pretrained_user_emb = dataset.get_preload_weight('userid')\n",
    "    pretrained_entity_emb = dataset.get_preload_weight('entityid')\n",
    "    pretrained_relation_emb = dataset.get_preload_weight('relationid')\n",
    "    \n",
    "    self.user_embedding = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_user_emb))\n",
    "    self.entity_embedding = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_entity_emb))\n",
    "    self.relation_embedding = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_relation_emb))\n",
    "```\n",
    "\n",
    "*Numpy*:\n",
    "```python\n",
    "    self.pretrained_user_emb = dataset.get_preload_weight('userid')\n",
    "    self.entity_embedding = dataset.get_preload_weight('entityid')\n",
    "    self.relation_embedding = dataset.get_preload_weight('relationid')\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
