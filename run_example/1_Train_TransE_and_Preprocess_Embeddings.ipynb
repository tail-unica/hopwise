{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from hopwise.data import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxEPlCFOm4X3"
   },
   "source": [
    "### 2. Embeddings preprocessing for PGPR\n",
    "\n",
    "In this phase we specify the checkpoint folder along its name and we load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kM8FQIKSm4X3"
   },
   "outputs": [],
   "source": [
    "checkpoint_name = \"saved/.....pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaUHYQUqm4X3"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_name, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ji6_3nAm4X4"
   },
   "source": [
    "**The detected embeddings are**\n",
    "\n",
    "\n",
    "Now we visualise which embeddings have been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0z37Lu9m4X4",
    "outputId": "f778fcfc-75d8-441d-f4a6-a7f8532a734f"
   },
   "outputs": [],
   "source": [
    "checkpoint[\"state_dict\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM8vSaLpm4X4"
   },
   "source": [
    "**Maybe you want to exclude some embeddings?**\n",
    "\n",
    "As I said before, there can be embeddings that we would exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRvj43mum4X4"
   },
   "outputs": [],
   "source": [
    "excluded = [\"relation_bias_embedding.weight\",\"norm_vec.weight\",\"proj_mat_e.weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwO95zfdm4X4"
   },
   "source": [
    "**The detected dataset is**\n",
    "\n",
    "Let's check which dataset has been used and where is the folder the dataset where the embeddings will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wTr_9T_Cm4X4",
    "outputId": "5eed2ff6-338d-4ea4-d427-4bb36e7eb823"
   },
   "outputs": [],
   "source": [
    "dataset_name = checkpoint[\"config\"][\"dataset\"]\n",
    "data_path = checkpoint[\"config\"][\"data_path\"]\n",
    "data_path\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GVjOZh3m4X4"
   },
   "source": [
    "**Create the mappings between embedding and original entity/relation/user**\n",
    "- Users have a mapping 1-1 so we don't need a mapping.\n",
    "\n",
    "- We suppose that indexing starts at 1. (tipically 0 is reserved for [PAD])\n",
    "\n",
    "*Note Francesca: Questi mapping sono necessari perch√© gli embeddings salvati nel modello sono semplicemente una matrice |users|x|items|, per questo abbiamo bisogno di mappare ogni riga (corrispondente all'utente) al corrispondente utente nel dataset originale. Stessa cosa per le entit√† e le relazioni. Per cui, se in ml1m un'entit√† si chiama 'mdajd12' allora nel file finale .entityemb che vedrai successivamente in questo notebook, la riga corrispondente all'entit√† iesima avr√† il suo nome nel dataset originale.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Pj7Bx0Pm4X4"
   },
   "outputs": [],
   "source": [
    "dataset = create_dataset(checkpoint[\"config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtAXZwIrm4X4",
    "outputId": "94207bcb-715f-4941-97dd-82eeb1ffa63b"
   },
   "outputs": [],
   "source": [
    "# create the reverse mapping\n",
    "uid2token = {id: token for token, id in dataset.field2token_id[\"user_id\"].items()}\n",
    "print(uid2token)\n",
    "eid2token = {id: token for token, id in dataset.field2token_id[\"tail_id\"].items()}\n",
    "print(eid2token)\n",
    "rid2token = {id: token for token, id in dataset.field2token_id[\"relation_id\"].items()}\n",
    "print(rid2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PdXKN_Am4X4"
   },
   "outputs": [],
   "source": [
    "# # add dummy relation, check kge code\n",
    "# rid2token[len(rid2token)] = 'ui_dummy_relation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmaGZ8NLm4X4"
   },
   "outputs": [],
   "source": [
    "assert len(eid2token.keys()) == checkpoint[\"state_dict\"][\"entity_embedding.weight\"].shape[0]\n",
    "assert len(rid2token.keys()) == checkpoint[\"state_dict\"][\"relation_embedding.weight\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zsbia4t_m4X4"
   },
   "source": [
    "*if the assertion check fails, make sure that you've trained the kge without adding dummy relations/entities explicitly when creating relation/entity embeddings!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWGzOXMLm4X5"
   },
   "source": [
    "### Create the new embeddings\n",
    "\n",
    "Now we take the embeddings, we create the mappings and we save the embeddings ready to be used with PGPR ü•≥üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjcpLUKdm4X5"
   },
   "outputs": [],
   "source": [
    "def format_embedding(weight, columns, emb_type):\n",
    "    weight = weight.detach().cpu().numpy()\n",
    "    new_emb_dict = {columns[0]: list(), columns[1]: list()}\n",
    "\n",
    "    if emb_type == \"entity\":\n",
    "        mapping = eid2token\n",
    "    elif emb_type == \"relation\":\n",
    "        mapping = rid2token\n",
    "    elif emb_type == \"user\":\n",
    "        mapping = uid2token\n",
    "\n",
    "    # Create index\n",
    "    new_emb_dict[columns[0]] = [mapping[id] if mapping is not None else id for id in range(1, weight.shape[0])]\n",
    "\n",
    "    # Create embedding\n",
    "    new_emb_dict[columns[1]] = [\" \".join(f\"{x}\" for x in row) for row in weight[1:]]\n",
    "\n",
    "    filename = f\"{dataset_name}.{emb_type}emb\"\n",
    "    df = pd.DataFrame(new_emb_dict)\n",
    "    print(f\"[+] Saving the new {dataset_name} {columns[0]} embedding in {data_path}/{filename}!\")\n",
    "    df.to_csv(os.path.join(data_path, filename), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Di7-GACam4X5",
    "outputId": "378b5e63-3a51-4ed0-80eb-315d1ed6a599"
   },
   "outputs": [],
   "source": [
    "\n",
    "for emb_name, emb in checkpoint[\"state_dict\"].items():\n",
    "    if emb_name in excluded:\n",
    "        continue\n",
    "    # What is? Entity? User? Relation? Item?\n",
    "    emb_type = emb_name.split(\"_\")[0]\n",
    "    # Create the new embedding file columns\n",
    "    columns = [f\"{emb_type}_embedding_id:token\", f\"{emb_type}_embedding:float_seq\"]\n",
    "    print(f\"[+] Formatting {emb_name} with columns {columns}\")\n",
    "    format_embedding(emb, columns, emb_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsDdrqUWm4X5"
   },
   "source": [
    "### Next?\n",
    "\n",
    "Let's check that everything is fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpsbkjVdm4X5"
   },
   "source": [
    "Now, in the dataset folder you can see the new saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JNWuMkam4X5",
    "outputId": "5512cc10-82ee-4354-94f9-c58018483e78"
   },
   "outputs": [],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-YUCLoMm4X5"
   },
   "source": [
    "### It's not yet over... final steps\n",
    "\n",
    "Some final checks to make sure everything runs smoothly...\n",
    "\n",
    "Once everything is run, you should have an output from previous boxes as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC-4n_uRm4X5"
   },
   "source": [
    "**We want to make sure that the dataset configuration is ok.**\n",
    "\n",
    "Suppose that the output of the format embedding phase is:\n",
    "\n",
    "```text\n",
    "    [+] Formatting user_embedding.weight with columns ['userid:token', 'user_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m userid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.useremb!\n",
    "    [+] Formatting entity_embedding.weight with columns ['entityid:token', 'entity_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m entityid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.entityemb!\n",
    "    [+] Formatting relation_embedding.weight with columns ['relationid:token', 'relation_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m relationid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.relationemb!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
