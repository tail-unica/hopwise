learning_rate: 2e-4
weight_decay: 0.01
warmup_steps: 250               # (int) Number of warmup steps for learning rate scheduler.
context_length: 24              # (int) Maximum length of the context.
embedding_size: 768             # (int) Size of the embeddings.
num_heads: 12                   # (int) Number of heads in the multi-head attention.
num_layers: 6                   # (int) Number of layers in the transformer.
ranker: BeamSearchSequenceScoreRanker # one of ['SampleSearchSequenceScoreRanker', 'BeamSearchSequenceScoreRanker', 'CumulativeSequenceScoreRanker']
logits_processor: ConstrainedBeamLogitsProcessorWordLevel # one of ['ConstrainedBeamLogitsProcessorWordLevel', 'ConstrainedLogitsProcessorWordLevel']
use_kg_token_types: True        # (bool) Whether to use token types for the knowledge graph.

path_sample_args:
    restrict_by_phase: False    # (bool) Whether to restrict the last item in the reasoning path by the used ids in the dataloader phase.

path_generation_args:
    paths_per_user: 30          # (int) Number of paths generated per user.
    num_beams: 50           # (int) Number of beams for beam search.
    num_beam_groups: 5      # (int) Number of groups for diverse beam search.
    diversity_penalty: 0.3  # (float) Diversity penalty for beam search.
    length_penalty: 0.0     # (float) Length penalty for beam search.
    do_sample: False        # (bool) Whether to use sampling ; use greedy decoding otherwise.