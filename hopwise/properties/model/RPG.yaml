# Config for the backbone decoder model (GPT2)
embedding_size: 448
layers: 2
heads: 4
embedding_size_inner_mlp: 1024
activation_function: gelu_new
resid_pdrop: 0.0
embd_pdrop: 0.5
attn_pdrop: 0.5
layer_norm_epsilon: 1e-12
initializer_range: 0.02
max_seq_length: 50

# Semantic ID configs (FAISS OPQ)
n_codebook: 16
codebook_size: 256
faiss_omp_num_threads: 32

# RPG configs
use_gcd: True # use graph constrained decoding
temperature: 0.03
n_beams: 10
n_edges: 100
propagation_steps: 2

#   decoding graph construction
chunk_size: 1024
num_workers: 64

eval_args:
  split: {'LS': 'valid_and_test'}
  order: TO
  mode: full
train_neg_sample_args: ~
reproducible: True
loss_type: 'CE'

# To load the pretrained embeddings (Typically from a SentenceEncoder) necessary to score the paths
additional_feat_suffix: [itememb]
load_col:
    inter: ['user_id', 'item_id', 'timestamp']
#    item: ['item_id']
    itememb: [item_embedding_id, item_embedding]
alias_of_item_id: [item_embedding_id]
preload_weight:
  item_embedding_id: item_embedding

gpu_id: 0
user_inter_num_interval: [5, inf]
item_inter_num_interval: [5, inf]
data_path: /home/recsysdatasets