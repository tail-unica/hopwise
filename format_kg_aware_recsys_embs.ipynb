{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Knowledge-Graph Embeddings for Hopwise `dataset.get_preload_weight()` function\n",
    "This notebook shows you how you can format kge methods embeddings to be loaded with `dataset.get_preload_weight`\n",
    "\n",
    "\n",
    "ðŸ“š [Load Pretrained Embedding Documentation](https://recbole.io/docs/user_guide/usage/load_pretrained_embedding.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from hopwise.data import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'saved/TransE-Feb-25-2025_12-22-01.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4178078/510420941.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_name)\n",
      "/home/asoccol/hopwise/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Embeddings detected are**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['user_embedding.weight', 'entity_embedding.weight', 'relation_embedding.weight'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you want to exclude some embeddings?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = ['relation_bias_embedding.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Dataset detected is**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml-100k'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = checkpoint['config']['dataset']\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Dataset folder detected is**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/asoccol/hopwise/hopwise/config/../dataset_example/ml-100k'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = checkpoint['config']['data_path']\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the mappings between embedding and original entity/relation/user**\n",
    "\n",
    "- Users have a mapping 1-1 so we don't need a mapping.\n",
    "\n",
    "- We suppose that indexing starts at 1. (tipically 0 is reserved for [PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(checkpoint['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'film.producer.film', 2: 'film.film.actor', 3: 'film.film_genre.films_in_this_genre', 4: 'film.film.genre', 5: 'film.writer.film', 6: 'film.film.award_nomination', 7: 'film.film.directed_by', 8: 'film.cinematographer.film', 9: 'film.film.produced_by', 10: 'film.film.subjects', 11: 'film.content_rating.film', 12: 'film.production_company.films', 13: 'film.film.written_by', 14: 'film.film.country', 15: 'film.film.language', 16: 'film.film_subject.films', 17: 'film.film.rating', 18: 'film.film.production_companies', 19: 'film.film.award_won', 20: 'film.director.film', 21: 'film.film.cinematography', 22: 'film.film.sequel', 23: 'film.film.prequel', 24: 'film.actor.film', 25: '[UI-Relation]'}\n"
     ]
    }
   ],
   "source": [
    "# create the reverse mapping\n",
    "eid2token = {id: token for token, id in dataset.field2token_id['head_id'].items()}\n",
    "rid2token = {id: token for token, id in dataset.field2token_id['relation_id'].items()}\n",
    "print(rid2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add dummy relation, check kge code\n",
    "# rid2token[len(rid2token)] = 'ui_dummy_relation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(eid2token.keys()) == checkpoint['state_dict']['entity_embedding.weight'].shape[0])\n",
    "assert (len(rid2token.keys()) == checkpoint['state_dict']['relation_embedding.weight'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*if the assertion check fails, make sure that you've trained the kge without adding dummy relations/entities explicitly when creating relation/entity embeddings!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_embedding(weight, columns, emb_type):\n",
    "    weight = weight.detach().cpu().numpy()\n",
    "    new_emb_dict = {columns[0]: list(), \n",
    "                    columns[1]: list() }\n",
    "    \n",
    "    if emb_type == 'entity':\n",
    "        mapping = eid2token    \n",
    "    elif emb_type == 'relation':\n",
    "        mapping = rid2token\n",
    "    else:\n",
    "        mapping = None\n",
    "    # Create index\n",
    "    new_emb_dict[columns[0]] = [mapping[id] if mapping is not None else id for id in range(1,len(weight))]\n",
    "\n",
    "    # Create embedding\n",
    "    new_emb_dict[columns[1]] = [\" \".join(f\"{x}\" for x in row) for row in weight[1:]]\n",
    "    \n",
    "    filename = f'{dataset_name}.{emb_type}emb'\n",
    "    df = pd.DataFrame(new_emb_dict)\n",
    "    print(f\"[+] Saving the new {dataset_name} {columns[0]} embedding in {data_path}/{filename}!\")\n",
    "    df.to_csv(os.path.join(data_path,filename), sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Formatting user_embedding.weight with columns ['userid:token', 'user_embedding:float_seq']\n",
      "[+] Saving the new ml-100k userid:token embedding in /home/asoccol/hopwise/hopwise/config/../dataset_example/ml-100k/ml-100k.useremb!\n",
      "[+] Formatting entity_embedding.weight with columns ['entityid:token', 'entity_embedding:float_seq']\n",
      "[+] Saving the new ml-100k entityid:token embedding in /home/asoccol/hopwise/hopwise/config/../dataset_example/ml-100k/ml-100k.entityemb!\n",
      "[+] Formatting relation_embedding.weight with columns ['relationid:token', 'relation_embedding:float_seq']\n",
      "[+] Saving the new ml-100k relationid:token embedding in /home/asoccol/hopwise/hopwise/config/../dataset_example/ml-100k/ml-100k.relationemb!\n"
     ]
    }
   ],
   "source": [
    "for emb_name, emb in checkpoint['state_dict'].items():\n",
    "    if emb_name in excluded:\n",
    "        continue\n",
    "    # What is? Entity? User? Relation? Item? \n",
    "    emb_type = emb_name.split(\"_\")[0]\n",
    "    # Create the new embedding file columns\n",
    "    columns = [f'{emb_type}id:token', f'{emb_type}_embedding:float_seq']\n",
    "    print(f\"[+] Formatting {emb_name} with columns {columns}\")\n",
    "    format_embedding(emb, columns, emb_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the dataset folder there are these file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ml-100k.user',\n",
       " 'ml-100k.relationemb',\n",
       " 'ml-100k.item',\n",
       " 'ml-100k.inter',\n",
       " 'ml-100k.useremb',\n",
       " 'ml-100k.link',\n",
       " 'ml-100k.entityemb',\n",
       " 'ml-100k.kg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to make sure that the dataset configuration is ok.**\n",
    "\n",
    "Suppose that the output of the format embedding phase is:\n",
    "\n",
    "```text\n",
    "    [+] Formatting user_embedding.weight with columns ['userid:token', 'user_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m userid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.useremb!\n",
    "    [+] Formatting entity_embedding.weight with columns ['entityid:token', 'entity_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m entityid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.entityemb!\n",
    "    [+] Formatting relation_embedding.weight with columns ['relationid:token', 'relation_embedding:float_seq']\n",
    "    [+] Saving the new ml-1m relationid:token embedding in /home/recsysdatasets/ml-1m/ml-1m.relationemb!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you should go to the dataset configuration file (in our case is in `hopwise/properties/dataset/ml-1m.yaml`) and add the new files to be loaded\n",
    "\n",
    "\n",
    "```text\n",
    "    additional_feat_suffix: [useremb, entityemb, relationemb]  \n",
    "    load_col:                                                  \n",
    "        useremb: [userid, user_embedding]\n",
    "        entityemb: [entityid, entity_embedding]\n",
    "        relationemb: [relationid, relation_embedding]\n",
    "    \n",
    "    alias_of_user_id: [userid]\n",
    "    alias_of_entity_id: [entityid]\n",
    "    alias_of_relation_id: [relationid]\n",
    "    \n",
    "    preload_weight:\n",
    "      userid: user_embedding\n",
    "      entityid: entity_embedding\n",
    "      relationid: relation_embedding\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in your code you should be able to access to pretrained embeddings in your model through:\n",
    "\n",
    "*Torch*\n",
    "```python\n",
    "    pretrained_user_emb = dataset.get_preload_weight('userid')\n",
    "    pretrained_entity_emb = dataset.get_preload_weight('entityid')\n",
    "    pretrained_relation_emb = dataset.get_preload_weight('relationid')\n",
    "    \n",
    "    self.user_embedding = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_user_emb))\n",
    "    self.entity_embedding = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_entity_emb))\n",
    "    self.relation_embedding = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_relation_emb))\n",
    "```\n",
    "\n",
    "*Numpy*:\n",
    "```python\n",
    "    self.pretrained_user_emb = dataset.get_preload_weight('userid')\n",
    "    self.entity_embedding = dataset.get_preload_weight('entityid')\n",
    "    self.relation_embedding = dataset.get_preload_weight('relationid')\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
